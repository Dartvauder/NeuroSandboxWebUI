# Как пользоваться:

#### Интерфейс имеет сорок один подраздел (некоторые с собственными подразделами) в семи основных разделах (Текст, Изображение, Видео, 3D, Аудио, Дополнительно и Интерфейс): LLM, TTS-STT, MMS, SeamlessM4Tv2, LibreTranslate, StableDiffusion, Kandinsky, Flux, HunyuanDiT, Lumina-T2X, Kolors, AuraFlow, Würstchen, DeepFloydIF, PixArt, CogView3-Plus, PlaygroundV2.5, Wav2Lip, LivePortrait, ModelScope, ZeroScope 2, CogVideoX, Latte, StableFast3D, Shap-E, Zero123Plus, StableAudio, AudioCraft, AudioLDM 2, SunoBark, RVC, UVR, Demucs, Upscale (Real-ESRGAN), FaceSwap, MetaData-Info, Wiki, Gallery, ModelDownloader, Settings и System. Выберите нужный и следуйте инструкциям ниже

# Текст:

### LLM:

1) Сначала загрузите ваши модели в папку: *inputs/text/llm_models*
2) Выберите вашу модель из выпадающего списка
3) Выберите тип модели
4) Настройте модель согласно нужным вам параметрам
5) Напечатайте (или произнесите) ваш запрос
6) Нажмите кнопку `Submit`, чтобы получить сгенерированный текстовый и аудио ответ
#### Дополнительно: вы можете включить режим `TTS`, выбрать нужный `голос` и `язык`, чтобы получить аудио ответ. Вы можете включить `multimodal` и загрузите изображение, видео и аудио файлы, чтобы получить их описание. Вы можете включить `websearch` для доступа в Интернет. Вы можете включить `libretranslate` для получения перевода. Вы можете включить `OpenParse` для работы с pdf файлами. Также вы можете выбрать модель `LORA` для улучшения генерации
#### Образцы голосов = *inputs/audio/voices*
#### LORA = *inputs/text/llm_models/lora*
#### Голос должен быть предварительно обработан (22050 кГц, моно, WAV)
#### Аватары LLM вы меняете в папке *avatars*

### TTS-STT:

1) Введите текст для преобразования текста в речь
2) Введите аудио для преобразования речи в текст
3) Нажмите кнопку `Submit`, чтобы получить сгенерированный текстовый и аудио ответ
#### Образцы голосов = *inputs/audio/voices*
#### Голос должен быть предварительно обработан (22050 кГц, моно, WAV)

### MMS (текст в речь и речь в текст):

1) Введите текст для преобразования текста в речь
2) Введите аудио для преобразования речи в текст
3) Нажмите кнопку `Submit`, чтобы получить сгенерированный текстовый или аудио ответ

### SeamlessM4Tv2:

1) Напечатайте (или произнесите) ваш запрос
2) Выберите исходный, целевой языки и язык набора данных
3) Настройте модель согласно нужным вам параметрам
4) Нажмите кнопку `Submit`, чтобы получить перевод

### LibreTranslate:

* Сначала вам нужно установить и запустить [LibreTranslate](https://github.com/LibreTranslate/LibreTranslate)
1) Выберите исходный и целевой языки
2) Нажмите кнопку `Submit`, чтобы получить перевод
#### Дополнительно: вы можете сохранить историю переводов, включив соответствующую кнопку

# Изображение:

### StableDiffusion - имеет двадцать четыре подраздела:

#### txt2img:

1) Сначала загрузите ваши модели в папку: *inputs/image/sd_models*
2) Выберите вашу модель из выпадающего списка
3) Выберите тип модели (`SD`, `SD2` или `SDXL`)
4) Настройте модель согласно нужным вам параметрам
5) Введите ваш запрос (+ и - для взвешивания промпта)
6) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение
#### Дополнительно: Вы можете выбрать ваши модели `vae`, `embedding` и `lora`, а также включить `MagicPrompt` для улучшения метода генерации
#### vae = *inputs/image/sd_models/vae*
#### lora = *inputs/image/sd_models/lora*
#### embedding = *inputs/image/sd_models/embedding*

#### img2img:

1) Сначала загрузите ваши модели в папку: *inputs/image/sd_models*
2) Выберите вашу модель из выпадающего списка
3) Выберите тип модели (`SD`, `SD2` или `SDXL`)
4) Настройте модель согласно нужным вам параметрам
5) Загрузите исходное изображение, с которым будет происходить генерация
6) Введите ваш запрос (+ и - для взвешивания промпта)
7) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение
#### Дополнительно: Вы можете выбрать ваши модели `vae`, `embedding` и `lora`, а также включить `MagicPrompt` для улучшения метода генерации
#### vae = *inputs/image/sd_models/vae*
#### lora = *inputs/image/sd_models/lora*
#### embedding = *inputs/image/sd_models/embedding*

#### depth2img:

1) Загрузите исходное изображение
2) Настройте модель согласно нужным вам параметрам
3) Введите ваш запрос (+ и - для взвешивания промпта)
4) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение

#### marigold:

1) Загрузите исходное изображение
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированные изображения глубины

#### pix2pix:

1) Загрузите исходное изображение
2) Настройте модель согласно нужным вам параметрам
3) Введите ваш запрос (+ и - для взвешивания промпта)
4) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение

#### controlnet:

1) Сначала загрузите ваши модели stable diffusion в папку: *inputs/image/sd_models*
2) Загрузите исходное изображение
3) Выберите ваши модели stable diffusion и controlnet из выпадающих списков
4) Настройте модели согласно нужным вам параметрам
5) Введите ваш запрос (+ и - для взвешивания промпта)
6) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение

#### upscale (latent):

1) Загрузите исходное изображение
2) Выберите вашу модель
3) Настройте модель согласно нужным вам параметрам
4) Нажмите кнопку `Submit`, чтобы получить увеличенное изображение

#### refiner (SDXL):

1) Загрузите исходное изображение
2) Нажмите кнопку `Submit`, чтобы получить улучшенное изображение

#### inpaint:

1) Сначала загрузите ваши модели в папку: *inputs/image/sd_models/inpaint*
2) Выберите вашу модель из выпадающего списка
3) Выберите тип модели (`SD`, `SD2` или `SDXL`)
4) Настройте модель согласно нужным вам параметрам
5) Загрузите изображение, с которым будет происходить генерация, в `initial image` и `mask image`
6) В `mask image` выберите кисть, затем палитру и измените цвет на `#FFFFFF`
7) Нарисуйте место для генерации и введите ваш запрос (+ и - для взвешивания промпта)
8) Нажмите кнопку `Submit`, чтобы получить изображение с внутренней заливкой
#### Дополнительно: Вы можете выбрать вашу модель `vae` для улучшения метода генерации
#### vae = *inputs/image/sd_models/vae*

#### outpaint:

1) Сначала загрузите ваши модели в папку: *inputs/image/sd_models/inpaint*
2) Выберите вашу модель из выпадающего списка
3) Выберите тип модели (`SD`, `SD2` или `SDXL`)
4) Настройте модель согласно нужным вам параметрам
5) Загрузите изображение, с которым будет происходить генерация, в `initial image`
6) Введите ваш запрос (+ и - для взвешивания промпта)
7) Нажмите кнопку `Submit`, чтобы получить изображение с внешней заливкой

#### gligen:

1) Сначала загрузите ваши модели в папку: *inputs/image/sd_models*
2) Выберите вашу модель из выпадающего списка
3) Выберите тип модели (`SD`, `SD2` или `SDXL`)
4) Настройте модель согласно нужным вам параметрам
5) Введите ваш запрос для промпта (+ и - для взвешивания промпта) и фразы GLIGEN (в "" для бокса)
6) Введите боксы GLIGEN (Например, [0.1387, 0.2051, 0.4277, 0.7090] для бокса)
7) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение

#### diffedit:

1) Введите ваш Исходный Промпт и Исходный Негативный Промпт для маскирования изображения
2) Введите ваш Целевой Промпт и Целевой Негативный Промпт для дифф-редактирования изображения
3) Загрузите исходное изображение
4) Настройте модель согласно нужным вам параметрам
5) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение

#### blip-diffusion:

1) Введите ваш Промпт
2) Загрузите исходное изображение
3) Введите ваши Условия и Целевые Объекты
4) Настройте модель согласно нужным вам параметрам
5) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение

#### animatediff:

1) Сначала загрузите ваши модели в папку: *inputs/image/sd_models*
2) Выберите вашу модель из выпадающего списка
3) Настройте модель согласно нужным вам параметрам
4) Введите ваш запрос (+ и - для взвешивания промпта)
5) Нажмите кнопку `Submit`, чтобы получить сгенерированную анимацию изображения
#### Дополнительно: вы можете выбрать motion LORA для управления вашей генерацией

#### hotshot-xl

1) Введите ваш запрос
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированное GIF-изображение

#### video:

1) Загрузите исходное изображение
2) Выберите вашу модель
3) Введите ваш запрос (для IV2Gen-XL)
4) Настройте модель согласно нужным вам параметрам
5) Нажмите кнопку `Submit`, чтобы получить видео из изображения

#### ldm3d:

1) Введите ваш запрос
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированные изображения

#### sd3 (txt2img, img2img, controlnet, inpaint):

1) Введите ваш запрос
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение
#### Опционально: Вы можете выбрать свои модели `lora` для улучшения метода генерации. Вы также можете использовать квантизированные модели, нажав кнопку `Enable quantize`, если у вас низкое количество видеопамяти. Однако вам нужно самостоятельно скачать модель: [CLIP-L](https://huggingface.co/Comfy-Org/stable-diffusion-3.5-fp8/blob/main/text_encoders/clip_l.safetensors), [CLIP-G](https://huggingface.co/Comfy-Org/stable-diffusion-3.5-fp8/blob/main/text_encoders/clip_g.safetensors) и [T5XXL](https://huggingface.co/Comfy-Org/stable-diffusion-3.5-fp8/blob/main/text_encoders/t5xxl_fp16.safetensors)
#### lora = *inputs/image/sd_models/lora*
#### Квантованные модели = *inputs/image/sd_models*

#### cascade:

1) Введите ваш запрос
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение

#### t2i-ip-adapter:

1) Загрузите исходное изображение
2) Выберите нужные вам опции
3) Нажмите кнопку `Submit`, чтобы получить модифицированное изображение

#### ip-adapter-faceid:

1) Загрузите исходное изображение
2) Выберите нужные вам опции
3) Нажмите кнопку `Submit`, чтобы получить модифицированное изображение

#### riffusion (текст-в-изображение, изображение-в-аудио, аудио-в-изображение):

- текст-в-изображение:
  - 1) Введите ваш запрос
    2) Настройте модель согласно нужным вам параметрам
    3) Нажмите кнопку `Submit`, чтобы получить сгенерированное изображение
- изображение-в-аудио:
  - 1) Загрузите исходное изображение
    2) Выберите нужные вам опции
    3) Нажмите кнопку `Submit`, чтобы получить аудио из изображения
- аудио-в-изображение:
  - 1) Загрузите исходное аудио
    2) Выберите нужные вам опции
    3) Нажмите кнопку `Submit`, чтобы получить изображение из аудио
   
### Kandinsky (txt2img, img2img, inpaint):

1) Введите ваш промпт
2) Выберите модель из выпадающего списка
3) Настройте модель согласно нужным вам параметрам
4) Нажмите `Submit`, чтобы получить сгенерированное изображение

### Flux (txt2img, img2img, inpaint, controlnet):

1) Введите ваш промпт
2) Выберите вашу модель
3) Настройте модель согласно нужным вам параметрам
4) Нажмите `Submit`, чтобы получить сгенерированное изображение
#### Дополнительно: Вы можете выбрать ваши модели `lora` для улучшения метода генерации. Вы также можете использовать квантованные модели, нажав на кнопку `Enable quantize`, если у вас мало видеопамяти, но вам нужно самостоятельно скачать модель: [FLUX.1-dev](https://huggingface.co/city96/FLUX.1-dev-gguf/tree/main) или [FLUX.1-schnell](https://huggingface.co/city96/FLUX.1-schnell-gguf/tree/main), а также [VAE](https://huggingface.co/black-forest-labs/FLUX.1-dev/blob/main/ae.safetensors), [CLIP](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/clip_l.safetensors) и [T5XXL](https://huggingface.co/comfyanonymous/flux_text_encoders/blob/main/t5xxl_fp16.safetensors)
#### lora = *inputs/image/flux-lora*
#### Квантованные модели = *inputs/image/quantize-flux*

### HunyuanDiT (txt2img, controlnet):

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение

### Lumina-T2X:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение

### Kolors (txt2img, img2img, ip-adapter-plus):

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение
#### Дополнительно: Вы можете выбрать ваши модели `lora` для улучшения метода генерации
#### lora = *inputs/image/kolors-lora*

### AuraFlow:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение
#### Дополнительно: Вы можете выбрать ваши модели `lora` и включить `AuraSR` для улучшения метода генерации
#### lora = *inputs/image/auraflow-lora*

### Würstchen:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение

### DeepFloydIF (txt2img, img2img, inpaint):

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение

### PixArt:

1) Введите ваш промпт
2) Выберите вашу модель
3) Настройте модель согласно нужным вам параметрам
4) Нажмите `Submit`, чтобы получить сгенерированное изображение

### CogView3-Plus:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение

### PlaygroundV2.5:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное изображение

# Видео:

### Wav2Lip:

1) Загрузите исходное изображение лица
2) Загрузите исходное аудио голоса
3) Настройте модель согласно нужным вам параметрам
4) Нажмите кнопку `Submit`, чтобы получить синхронизацию губ

### LivePortrait:

1) Загрузите исходное изображение лица
2) Загрузите исходное видео движения лица
3) Нажмите кнопку `Submit`, чтобы получить анимированное изображение лица

### ModelScope:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное видео

### ZeroScope 2:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное видео

### CogVideoX (text2video, image2video, video2video):

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное видео

### Latte:

1) Введите ваш промпт
2) Настройте модель согласно нужным вам параметрам
3) Нажмите `Submit`, чтобы получить сгенерированное видео

# 3D:

### StableFast3D:

1) Загрузите исходное изображение
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированный 3D объект

### Shap-E:

1) Введите ваш запрос или загрузите исходное изображение
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированный 3D объект

### Zero123Plus:

1) Загрузите исходное изображение
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированное 3D вращение изображения

# Аудио:

### StableAudio:

1) Настройте модель согласно нужным вам параметрам
2) Введите ваш запрос
3) Нажмите кнопку `Submit`, чтобы получить сгенерированное аудио

### AudioCraft:

1) Выберите модель из выпадающего списка
2) Выберите тип модели (`musicgen`, `audiogen` или `magnet`)
3) Настройте модель согласно нужным вам параметрам
4) Введите ваш запрос
5) (Дополнительно) загрузите исходное аудио, если вы используете модель `melody`
6) Нажмите кнопку `Submit`, чтобы получить сгенерированное аудио
#### Дополнительно: Вы можете включить `multiband diffusion` для улучшения сгенерированного аудио

### AudioLDM 2:

1) Выберите модель из выпадающего списка
2) Настройте модель согласно нужным вам параметрам
3) Введите ваш запрос
4) Нажмите кнопку `Submit`, чтобы получить сгенерированное аудио

### SunoBark:

1) Введите ваш запрос
2) Настройте модель согласно нужным вам параметрам
3) Нажмите кнопку `Submit`, чтобы получить сгенерированный аудио ответ

### RVC:

1) Сначала загрузите ваши модели в папку: *inputs/audio/rvc_models*
2) Загрузите исходное аудио
3) Выберите вашу модель из выпадающего списка
4) Настройте модель согласно нужным вам параметрам
5) Нажмите кнопку `Submit`, чтобы получить сгенерированное клонирование голоса

### UVR:

1) Загрузите исходное аудио для разделения
2) Нажмите кнопку `Submit`, чтобы получить разделенное аудио

### Demucs:

1) Загрузите исходное аудио для разделения
2) Нажмите кнопку `Submit`, чтобы получить разделенное аудио

# Дополнительно (Изображение, Видео, Аудио):

1) Загрузите исходный файл
2) Выберите нужные вам опции
3) Нажмите кнопку `Submit`, чтобы получить модифицированный файл

### Увеличение масштаба (Real-ESRGAN):

1) Загрузите исходное изображение
2) Выберите вашу модель
3) Настройте модель согласно нужным вам параметрам
4) Нажмите кнопку `Submit`, чтобы получить увеличенное изображение

### FaceSwap:

1) Загрузите исходное изображение лица
2) Загрузите целевое изображение или видео лица
3) Выберите нужные вам опции
4) Нажмите кнопку `Submit`, чтобы получить изображение с замененным лицом
#### Дополнительно: вы можете включить FaceRestore для увеличения масштаба и восстановления вашего изображения/видео лица

### MetaData-Info:

1) Загрузите сгенерированный файл
2) Нажмите кнопку `Submit`, чтобы получить информацию о метаданных файла

# Интерфейс:

### Wiki:

* Здесь вы можете просмотреть онлайн или офлайн вики проекта

### Gallery:

* Здесь вы можете просмотреть файлы из директории outputs

### ModelDownloader:

* Здесь вы можете скачать модели

### Settings: 

* Здесь вы можете изменить настройки приложения

### System: 

* Здесь вы можете увидеть показатели датчиков вашего компьютера

### Дополнительная информация:

1) Все генерации сохраняются в папке *outputs*. Вы можете открыть папку *outputs* с помощью кнопки `Outputs`
2) Вы можете выключить приложение с помощью кнопки `Close terminal` и перезагрузить выпадающие списки моделей с помощью кнопки `Reload interface`

## Где я могу получить модели и голоса?

* Модели LLM можно взять с [HuggingFace](https://huggingface.co/models) или из ModelDownloader внутри интерфейса
* Модели StableDiffusion, vae, inpaint, embedding и lora можно взять с [CivitAI](https://civitai.com/models) или из ModelDownloader внутри интерфейса
* Модели RVC можно взять с [VoiceModels](https://voice-models.com) или из ModelDownloader внутри интерфейса
* Модели StableAudio, AudioCraft, AudioLDM 2, TTS, Whisper, MMS, SeamlessM4Tv2, Wav2Lip, LivePortrait, SunoBark, MoonDream2, Upscalers (Latent и Real-ESRGAN), Refiner, GLIGEN, DiffEdit, BLIP-Diffusion, Depth, Marigold, Pix2Pix, Controlnet, AnimateDiff, HotShot-XL, Videos, LDM3D, SD3, Cascade, T2I-IP-ADAPTER, IP-Adapter-FaceID, Riffusion, Rembg, Roop, CodeFormer, DDColor, PixelOE, Real-ESRGAN, StableFast3D, Shap-E, Zero123Plus, UVR, Demucs, Kandinsky, Flux, HunyuanDiT, Lumina-T2X, Kolors, AuraFlow, AuraSR, Würstchen, DeepFloydIF, PixArt, CogView3-Plus, PlaygroundV2.5, ModelScope, ZeroScope 2, CogVideoX, MagicPrompt, Latte и Multiband diffusion загружаются автоматически в папку *inputs* при их использовании
* Голоса вы можете взять где угодно. Запишите свой или возьмите запись из Интернета. Или просто используйте те, которые уже есть в проекте. Главное, чтобы они были предварительно обработаны!

## Дорожная карта и багтрекер:

[DiscussionLink](https://github.com/Dartvauder/NeuroSandboxWebUI/discussions/248)
